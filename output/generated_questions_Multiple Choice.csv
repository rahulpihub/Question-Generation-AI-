Topic,Subtopic,Level,Question Type,Question,Option 1,Option 2,Option 3,Option 4,Answer
,data engineers,Intermediate,Multiple Choice,,Databricks Runtime ML, Databricks Runtime Genomics, Databricks Runtime for Photon, Databricks Runtime,Databricks Runtime
databricks,data engineers,Intermediate,,What is the primary language used for data manipulation and transformation within Databricks by data engineers?,, Python, Scala, SQL,Python
databricks,data engineers,,Multiple Choice,Delta Live Tables (DLT) in Databricks are primarily used for what purpose by data engineers?,Building machine learning models,, Orchestrating data pipelines, Managing infrastructure,Orchestrating data pipelines
databricks,,Intermediate,Multiple Choice,Which Databricks component allows data engineers to schedule and automate production data pipelines?,Databricks SQL, Databricks Workflows,, Delta Live Tables,Databricks Workflows
,data engineers,Intermediate,Multiple Choice,What is the recommended way for data engineers to handle sensitive information like database credentials in Databricks?,Hardcoding them in notebooks, Storing them in plain text files, Using Databricks Secrets,,Using Databricks Secrets
databricks,data engineers,Intermediate,Multiple Choice,How do data engineers typically interact with data stored in cloud storage services like AWS S3 or Azure Blob Storage from Databricks?,Directly accessing files via file paths, Mounting the storage as a virtual file system, Using specialized APIs for each storage service, Copying data to local disk,Mounting the storage as a virtual file system
databricks,data engineers,Intermediate,Multiple Choice,,CSV, JSON, Parquet, Avro,Parquet
databricks,data engineers,Intermediate,Multiple Choice,What Databricks feature allows data engineers to define data quality rules and expectations as part of their ETL pipelines?,Databricks SQL, Databricks Unity Catalog,, Databricks Runtime,Delta Live Tables expectations
CSV,data engineers,Intermediate,Multiple Choice,"In Databricks, what is the purpose of an ""Auto Loader""?",To automatically generate ETL code, To incrementally load data from cloud storage, To load data from relational databases, To optimize query performance,To incrementally load data from cloud storage
databricks,data engineers,Intermediate,Multiple Choice,,Question: When working with large datasets in Databricks, what technique can data engineers utilize to optimize performance and reduce memory usage during transformations?,,,"Options: Broadcasting small tables, Using Python UDFs extensively, Storing data in CSV format, Avoiding caching"
